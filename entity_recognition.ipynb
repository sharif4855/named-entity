{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "named entity recognition NLP CC.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ut-XDow6TECx",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "----------------------Task 1----------------------\n",
        "\n",
        "Build a parser that can extract building/business name from a given address.\n",
        "\n",
        "building/business name might appears in various format within an address.\n",
        "\n",
        "Example input address and respective expected building/business name are given in 'sample_building_name_extracted.csv' file\n",
        "\n",
        "Sample Road Name List also provided in 'names_sample.csv'\n",
        "\n",
        "Based on that road list you can train custom road model to extract building/business name given in 'user_raw_data.csv'\n",
        "\n",
        "-------------------------Task 2-----------------------------\n",
        "\n",
        "Build a parser that can extract building/business name from a given address\n",
        "\n",
        "building/business name might appears in various format within an address.\n",
        "\n",
        "Example input address and respective expected road no/road name are given in 'sample_road_name_extracted.csv' file\n",
        "\n",
        "Sample Road Name List also provided in 'roads.csv'\n",
        "\n",
        "Based on that road list you can train custom road model to extract road no/road name given in 'user_raw_data.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsGptZC2TEC0",
        "colab_type": "code",
        "outputId": "0fe39d89-d3b7-4128-fe22-3b4fae890880",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "# Install extra-dependencies\n",
        "\n",
        "#for downloading extended dependencis\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "! pip -q install git+https://www.github.com/keras-team/keras-contrib.git sklearn-crfsuite\n",
        "! pip install keras==2.2.4"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting keras==2.2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 2.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.17.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (2.8.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.0.8)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.1.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (3.13)\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.2.5\n",
            "    Uninstalling Keras-2.2.5:\n",
            "      Successfully uninstalled Keras-2.2.5\n",
            "Successfully installed keras-2.2.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jSZ2EQMTEC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing all the necessary liaberies\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import WordPunctTokenizer, word_tokenize\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Model, Input\n",
        "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
        "from keras_contrib.layers import CRF\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWo1J_RrU_S0",
        "colab_type": "code",
        "outputId": "5945efe1-85c3-4832-9a97-ac1983c4425b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "# mounting google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/My Drive/Tasks/')"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewlSMKc9Htqc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load all the data for TASK 2:\n",
        "t2_user_name = pd.read_csv('Task2/roads.csv')\n",
        "t2_road = pd.read_csv('Task2/sample_road_name_extracted.csv')\n",
        "t2_raw_user = pd.read_excel('Task2/user_raw_data.xlsx')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKrjLKQlH0At",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Function to pre process the whole dataset. \n",
        "\n",
        "Taking input the dataframe and returning a list\n",
        "\"\"\"\n",
        "def preProcessData(user_data):\n",
        "  #replacing all \\n to space and other uncessary info from scenence and converted to them in lowercase letter\n",
        "  word_list = []\n",
        "  user_data = user_data.replace({'\\n'},'', regex=True)\n",
        "  user_data = user_data.loc[:, \"address\"].str.lower()\n",
        "  \n",
        "  for i in range(len(user_data)):\n",
        "    tokenized_data = nltk.word_tokenize(user_data.iloc[i])\n",
        "    word_list.append(tokenized_data)\n",
        "  \n",
        "  return word_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiXuSwIuH5UU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#receiving data from preProcessData function\n",
        "processed_data = preProcessData(t2_raw_user)\n",
        "\n",
        "word_list = []\n",
        "sen_list = []\n",
        "counter = 0\n",
        "#loping through all the words for converting it into a list\n",
        "for i in processed_data:\n",
        "  counter = counter + 1\n",
        "  sentence = 'sentence-' + str(counter)\n",
        "  for r in i:\n",
        "    word_list.append(r)\n",
        "    sen_list.append(sentence)\n",
        "\n",
        "dic = {'sentence':sen_list, 'word':word_list}\n",
        "data = pd.DataFrame(dic)\n",
        "\n",
        "data.to_excel(\"pre_processed_output_data.xlsx\", index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYjAR3NeTEC8",
        "colab_type": "text"
      },
      "source": [
        "## Hyperprameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcflPcxiTEC9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 256  # Number of examples used in each iteration\n",
        "EPOCHS = 25  # Number of passes through entire dataset\n",
        "MAX_LEN = 40  # Max length of review (in words)\n",
        "EMBEDDING = 40  # Dimension of word embedding vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln92tF3cTEDC",
        "colab_type": "code",
        "outputId": "a64f4912-ae05-4d6c-aca3-41d575cceb00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#Data preview\n",
        "data = pd.read_excel(\"test_file.xlsx\")\n",
        "print(\"Number of sentences: \", len(data.groupby(['Sentence #'])))\n",
        "\n",
        "words = list(set(data[\"Word\"].values))\n",
        "n_words = len(words)\n",
        "print(\"Number of words in the dataset: \", n_words)\n",
        "\n",
        "tags = list(set(data[\"Tag\"].values))\n",
        "print(\"Tags:\", tags)\n",
        "n_tags = len(tags)\n",
        "print(\"Number of Labels: \", n_tags)\n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sentences:  1450\n",
            "Number of words in the dataset:  3924\n",
            "Tags: ['B-BLD', 'O', 'I-RAD', 'B-RAD', 'I-BLD']\n",
            "Number of Labels:  5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "189JTeNoTEDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentenceGetter(object):\n",
        "    \"\"\"Class to Get the sentence in this format:\"\"\"\n",
        "\n",
        "    def __init__(self, data):\n",
        "        self.n_sent = 1\n",
        "        self.data = data\n",
        "        self.empty = False\n",
        "        healper_func = lambda s: [(w, t) for w, t in zip(s[\"Word\"].values.tolist(), s[\"Tag\"].values.tolist())]\n",
        "        self.grouped = self.data.groupby(\"Sentence #\").apply(healper_func)\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "    \n",
        "    def get_next(self):\n",
        "        \"\"\"Return one sentence\"\"\"\n",
        "        try:\n",
        "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
        "            self.n_sent += 1\n",
        "            return s\n",
        "        except:\n",
        "            return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_k6hyseTEDQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "using the `word2idx` dictionary to convert each word to a corresponding integer ID \n",
        "\"\"\"\n",
        "word2idx = {w: i + 2 for i, w in enumerate(words)}\n",
        "word2idx[\"UNK\"] = 1 # Unknown words\n",
        "word2idx[\"PAD\"] = 0 # Padding\n",
        "idx2word = {i: w for w, i in word2idx.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gs2u66HfTEDU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Using the `tag2idx` to convert each tags to a corresponding integer ID\n",
        "\"\"\"\n",
        "tag2idx = {t: i+1 for i, t in enumerate(tags)}\n",
        "tag2idx[\"PAD\"] = 0\n",
        "idx2tag = {i: w for w, i in tag2idx.items()}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq5sb__3TEDX",
        "colab_type": "code",
        "outputId": "c45da32e-aaab-4270-ae72-4cbe0f8e083e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"\n",
        "To feed the text into our Bi-LSTM-CRF, all texts should be the same length. \n",
        "used the sequence.pad_sequences() method and MAX_LEN variable.\n",
        "All texts longer than MAX_LEN are truncated and shorter texts are padded to get them to the same length\n",
        "\"\"\"\n",
        "# Convert each sentence from list of Token to list of word_index\n",
        "X = [[word2idx[w[0]] for w in s] for s in sentences]\n",
        "# Padding each sentence to have the same lenght\n",
        "X = pad_sequences(maxlen=MAX_LEN, sequences=X, padding=\"post\", value=word2idx[\"PAD\"])\n",
        "\n",
        "# Convert Tag/Label to tag_index\n",
        "y = [[tag2idx[w[1]] for w in s] for s in sentences]\n",
        "# Padding each sentence to have the same lenght\n",
        "y = pad_sequences(maxlen=MAX_LEN, sequences=y, padding=\"post\", value=tag2idx[\"PAD\"])\n",
        "\n",
        "# One-Hot encode\n",
        "y = [to_categorical(i, num_classes=n_tags+1) for i in y]  # n_tags+1(PAD)\n",
        "\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.1)\n",
        "X_tr.shape, X_te.shape, np.array(y_tr).shape, np.array(y_te).shape"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1305, 40), (145, 40), (1305, 40, 6), (145, 40, 6))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz8IrBxtTEDc",
        "colab_type": "code",
        "outputId": "d0d2e848-63e8-44e6-8d23-e0ff58cd5c55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "# Model definition\n",
        "input = Input(shape=(MAX_LEN,))\n",
        "model = Embedding(input_dim=n_words+2, output_dim=EMBEDDING, input_length=MAX_LEN, mask_zero=True)(input)  # default: 20-dim embedding\n",
        "model = Bidirectional(LSTM(units=50, return_sequences=True, recurrent_dropout=0.1))(model)  # variational biLSTM\n",
        "model = TimeDistributed(Dense(50, activation=\"relu\"))(model)\n",
        "crf = CRF(n_tags+1)  # CRF layer, n_tags+1(PAD)\n",
        "out = crf(model)  # output\n",
        "\n",
        "model = Model(input, out)\n",
        "model.compile(optimizer=\"rmsprop\", loss=crf.loss_function, metrics=[crf.accuracy])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_contrib/layers/crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n",
            "  warnings.warn('CRF.loss_function is deprecated '\n",
            "/usr/local/lib/python3.6/dist-packages/keras_contrib/layers/crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n",
            "  warnings.warn('CRF.accuracy is deprecated and it '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         (None, 40)                0         \n",
            "_________________________________________________________________\n",
            "embedding_6 (Embedding)      (None, 40, 40)            157040    \n",
            "_________________________________________________________________\n",
            "bidirectional_6 (Bidirection (None, 40, 100)           36400     \n",
            "_________________________________________________________________\n",
            "time_distributed_6 (TimeDist (None, 40, 50)            5050      \n",
            "_________________________________________________________________\n",
            "crf_6 (CRF)                  (None, 40, 6)             354       \n",
            "=================================================================\n",
            "Total params: 198,844\n",
            "Trainable params: 198,844\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdgbHJQ_TEDi",
        "colab_type": "code",
        "outputId": "2b5b7c45-4aae-4d29-b1f1-b524c41de342",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "source": [
        "history = model.fit(X_tr, np.array(y_tr), batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.1, verbose=2)\n",
        "\n",
        "#to save the model\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"model_road_bld.h5\")"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1174 samples, validate on 131 samples\n",
            "Epoch 1/25\n",
            " - 1s - loss: 7.9128 - crf_viterbi_accuracy: 0.9334 - val_loss: 7.7079 - val_crf_viterbi_accuracy: 0.8842\n",
            "Epoch 2/25\n",
            " - 2s - loss: 7.9094 - crf_viterbi_accuracy: 0.9337 - val_loss: 7.6865 - val_crf_viterbi_accuracy: 0.8813\n",
            "Epoch 3/25\n",
            " - 2s - loss: 7.8997 - crf_viterbi_accuracy: 0.9405 - val_loss: 7.7160 - val_crf_viterbi_accuracy: 0.8865\n",
            "Epoch 4/25\n",
            " - 2s - loss: 7.8929 - crf_viterbi_accuracy: 0.9433 - val_loss: 7.8175 - val_crf_viterbi_accuracy: 0.8805\n",
            "Epoch 5/25\n",
            " - 1s - loss: 7.8946 - crf_viterbi_accuracy: 0.9402 - val_loss: 7.7176 - val_crf_viterbi_accuracy: 0.8805\n",
            "Epoch 6/25\n",
            " - 1s - loss: 7.8830 - crf_viterbi_accuracy: 0.9469 - val_loss: 7.7433 - val_crf_viterbi_accuracy: 0.8827\n",
            "Epoch 7/25\n",
            " - 1s - loss: 7.8774 - crf_viterbi_accuracy: 0.9491 - val_loss: 7.7644 - val_crf_viterbi_accuracy: 0.8857\n",
            "Epoch 8/25\n",
            " - 1s - loss: 7.8711 - crf_viterbi_accuracy: 0.9537 - val_loss: 7.7836 - val_crf_viterbi_accuracy: 0.8835\n",
            "Epoch 9/25\n",
            " - 1s - loss: 7.8683 - crf_viterbi_accuracy: 0.9519 - val_loss: 7.8696 - val_crf_viterbi_accuracy: 0.8835\n",
            "Epoch 10/25\n",
            " - 2s - loss: 7.8658 - crf_viterbi_accuracy: 0.9527 - val_loss: 7.8121 - val_crf_viterbi_accuracy: 0.8850\n",
            "Epoch 11/25\n",
            " - 2s - loss: 7.8602 - crf_viterbi_accuracy: 0.9548 - val_loss: 7.7960 - val_crf_viterbi_accuracy: 0.8842\n",
            "Epoch 12/25\n",
            " - 2s - loss: 7.8561 - crf_viterbi_accuracy: 0.9582 - val_loss: 7.8319 - val_crf_viterbi_accuracy: 0.8820\n",
            "Epoch 13/25\n",
            " - 2s - loss: 7.8530 - crf_viterbi_accuracy: 0.9608 - val_loss: 7.8021 - val_crf_viterbi_accuracy: 0.8783\n",
            "Epoch 14/25\n",
            " - 1s - loss: 7.8454 - crf_viterbi_accuracy: 0.9652 - val_loss: 7.8419 - val_crf_viterbi_accuracy: 0.8835\n",
            "Epoch 15/25\n",
            " - 1s - loss: 7.8411 - crf_viterbi_accuracy: 0.9670 - val_loss: 7.8309 - val_crf_viterbi_accuracy: 0.8798\n",
            "Epoch 16/25\n",
            " - 1s - loss: 7.8393 - crf_viterbi_accuracy: 0.9679 - val_loss: 7.9228 - val_crf_viterbi_accuracy: 0.8857\n",
            "Epoch 17/25\n",
            " - 2s - loss: 7.8334 - crf_viterbi_accuracy: 0.9713 - val_loss: 7.9284 - val_crf_viterbi_accuracy: 0.8842\n",
            "Epoch 18/25\n",
            " - 1s - loss: 7.8327 - crf_viterbi_accuracy: 0.9693 - val_loss: 7.8550 - val_crf_viterbi_accuracy: 0.8820\n",
            "Epoch 19/25\n",
            " - 1s - loss: 7.8268 - crf_viterbi_accuracy: 0.9732 - val_loss: 7.9626 - val_crf_viterbi_accuracy: 0.8842\n",
            "Epoch 20/25\n",
            " - 1s - loss: 7.8236 - crf_viterbi_accuracy: 0.9765 - val_loss: 7.9063 - val_crf_viterbi_accuracy: 0.8865\n",
            "Epoch 21/25\n",
            " - 1s - loss: 7.8194 - crf_viterbi_accuracy: 0.9775 - val_loss: 7.9853 - val_crf_viterbi_accuracy: 0.8872\n",
            "Epoch 22/25\n",
            " - 1s - loss: 7.8160 - crf_viterbi_accuracy: 0.9784 - val_loss: 7.9867 - val_crf_viterbi_accuracy: 0.8880\n",
            "Epoch 23/25\n",
            " - 2s - loss: 7.8108 - crf_viterbi_accuracy: 0.9815 - val_loss: 7.9822 - val_crf_viterbi_accuracy: 0.8857\n",
            "Epoch 24/25\n",
            " - 2s - loss: 7.8140 - crf_viterbi_accuracy: 0.9809 - val_loss: 7.9141 - val_crf_viterbi_accuracy: 0.8730\n",
            "Epoch 25/25\n",
            " - 2s - loss: 7.8073 - crf_viterbi_accuracy: 0.9829 - val_loss: 7.9848 - val_crf_viterbi_accuracy: 0.8895\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dmrdma1FTEDl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Eval\n",
        "pred_cat = model.predict(X_te)\n",
        "pred = np.argmax(pred_cat, axis=-1)\n",
        "y_te_true = np.argmax(y_te, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-i3UwWMTEDp",
        "colab_type": "code",
        "outputId": "415d0663-3174-4aac-eec6-cbb6bb18b2be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# Convert the index to tag\n",
        "pred_tag = [[idx2tag[i] for i in row] for row in pred]\n",
        "y_te_true_tag = [[idx2tag[i] for i in row] for row in y_te_true] \n",
        "\n",
        "report = flat_classification_report(y_pred=pred_tag, y_true=y_te_true_tag)\n",
        "print(report)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-BLD       0.17      0.08      0.11        26\n",
            "       B-RAD       0.35      0.34      0.35        32\n",
            "       I-BLD       0.20      0.18      0.19        45\n",
            "       I-RAD       0.56      0.45      0.50        49\n",
            "           O       0.93      0.95      0.94      1228\n",
            "         PAD       1.00      1.00      1.00      4420\n",
            "\n",
            "    accuracy                           0.97      5800\n",
            "   macro avg       0.54      0.50      0.51      5800\n",
            "weighted avg       0.97      0.97      0.97      5800\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYCbvDzfKQbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}